# 数学01: ベクトルと行列の基礎

## 目次
- [1.1 はじめに](#11-はじめに)
- [1.2 ベクトル（Vector）](#12-ベクトルvector)
  - [1.2.1 ベクトルとは](#121-ベクトルとは)
  - [1.2.2 ベクトルの表記](#122-ベクトルの表記)
  - [1.2.3 ベクトルの次元](#123-ベクトルの次元)
  - [1.2.4 PyTorchでのベクトル](#124-pytorchでのベクトル)
- [1.3 行列（Matrix）](#13-行列matrix)
  - [1.3.1 行列とは](#131-行列とは)
  - [1.3.2 行列の形状](#132-行列の形状)
  - [1.3.3 PyTorchでの行列](#133-pytorchでの行列)
- [1.4 テンソル（Tensor）](#14-テンソルtensor)
  - [1.4.1 多次元配列](#141-多次元配列)
  - [1.4.2 テンソルの形状](#142-テンソルの形状)
- [1.5 基本的な演算](#15-基本的な演算)
  - [1.5.1 要素ごとの演算](#151-要素ごとの演算)
  - [1.5.2 ベクトルの内積](#152-ベクトルの内積)
  - [1.5.3 行列とベクトルの積](#153-行列とベクトルの積)
  - [1.5.4 行列と行列の積](#154-行列と行列の積)
- [1.6 転置（Transpose）](#16-転置transpose)
- [1.7 ブロードキャスティング](#17-ブロードキャスティング)
- [1.8 nanochatでの使用例](#18-nanochatでの使用例)
- [1.9 まとめ](#19-まとめ)

---

## 1.1 はじめに

深層学習では、**ベクトル**と**行列**が基本的なデータ構造です。テキスト、画像、音声などあらゆるデータは、最終的にベクトルや行列として表現されます。

この章では、深層学習に必要なベクトルと行列の基礎知識を、nanochatプロジェクトの具体例とともに学びます。

**前提知識**：
- 高校数学レベルの算数（四則演算、累乗など）
- 配列の概念

---

## 1.2 ベクトル（Vector）

### 1.2.1 ベクトルとは

**ベクトル**は、数値を一列に並べたものです。

```
例1: 3次元ベクトル
v = [2, 5, 1]

例2: 単語の埋め込みベクトル（5次元）
"cat" → [0.2, -0.5, 0.8, 0.1, -0.3]
```

ベクトルは以下のように使われます：
- **テキスト**: 各単語を数値ベクトルで表現（埋め込み）
- **画像**: ピクセル値をベクトル化
- **特徴量**: 中間層の出力

### 1.2.2 ベクトルの表記

数学的には、ベクトルは以下のように表記されます：

```
縦ベクトル（列ベクトル）:
    ┌   ┐
    │ 2 │
v = │ 5 │
    │ 1 │
    └   ┘

横ベクトル（行ベクトル）:
v = [ 2  5  1 ]
```

プログラミングでは通常、配列やリストとして表現します：

```python
# Pythonのリスト
v = [2, 5, 1]

# NumPy配列
import numpy as np
v = np.array([2, 5, 1])

# PyTorchテンソル
import torch
v = torch.tensor([2, 5, 1])
```

### 1.2.3 ベクトルの次元

ベクトルの**次元**は、要素の個数です。

```
1次元ベクトル: [5]                    # スカラーに近い
2次元ベクトル: [3, 7]                 # 平面上の点
3次元ベクトル: [2, 5, 1]              # 空間上の点
768次元ベクトル: [0.1, -0.2, ..., 0.5]  # 単語の埋め込み（GPT-2など）
```

nanochatでは、モデルの次元数（`n_embd`）に応じて、各トークンが高次元ベクトルで表現されます。

### 1.2.4 PyTorchでのベクトル

```python
import torch

# 1次元テンソル（ベクトル）
v = torch.tensor([2.0, 5.0, 1.0])
print(v)        # tensor([2., 5., 1.])
print(v.shape)  # torch.Size([3])  - 3次元ベクトル
print(v.ndim)   # 1  - 1次元配列
```

**重要な区別**：
- **テンソルの次元数（ndim）**: 配列の階層の深さ（1次元、2次元、3次元...）
- **ベクトルの次元**: 要素の個数（3次元ベクトル、768次元ベクトルなど）

```python
# ベクトルの次元 = 5、テンソルの次元数 = 1
v = torch.tensor([1, 2, 3, 4, 5])
print(v.shape)  # torch.Size([5])
print(v.ndim)   # 1
```

---

## 1.3 行列（Matrix）

### 1.3.1 行列とは

**行列**は、数値を長方形に並べたものです。2次元の数値配列とも言えます。

```
例: 3行4列の行列

    ┌                      ┐
    │  1   2   3   4       │
M = │  5   6   7   8       │
    │  9  10  11  12       │
    └                      ┘
```

行列は以下のように使われます：
- **重み行列**: ニューラルネットワークの各層のパラメータ
- **データバッチ**: 複数のサンプルをまとめて処理
- **画像**: ピクセルの2次元配列

### 1.3.2 行列の形状

行列の**形状（Shape）**は、(行数, 列数) で表されます。

```
3行4列の行列: (3, 4)

    ┌                      ┐
    │  1   2   3   4       │ ← 1行目
M = │  5   6   7   8       │ ← 2行目
    │  9  10  11  12       │ ← 3行目
    └                      ┘
      ↑   ↑   ↑   ↑
    1列 2列 3列 4列
```

**特殊な行列**：
```
正方行列（行数 = 列数）:
    ┌          ┐
    │ 1  2  3  │
    │ 4  5  6  │  (3, 3)
    │ 7  8  9  │
    └          ┘

行ベクトル（1行の行列）:
    [ 1  2  3  4 ]  (1, 4)

列ベクトル（1列の行列）:
    ┌   ┐
    │ 1 │
    │ 2 │  (3, 1)
    │ 3 │
    └   ┘
```

### 1.3.3 PyTorchでの行列

```python
import torch

# 2次元テンソル（行列）
M = torch.tensor([
    [1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]
])

print(M)
# tensor([[ 1,  2,  3,  4],
#         [ 5,  6,  7,  8],
#         [ 9, 10, 11, 12]])

print(M.shape)  # torch.Size([3, 4])  - 3行4列
print(M.ndim)   # 2  - 2次元配列
```

**行列の要素へのアクセス**：

```python
# インデックスは0から始まる
M[0, 0]  # 1  - 1行1列目
M[0, 1]  # 2  - 1行2列目
M[1, 2]  # 7  - 2行3列目

# 行全体を取得
M[0]     # tensor([1, 2, 3, 4])  - 1行目全体

# 列全体を取得
M[:, 0]  # tensor([1, 5, 9])  - 1列目全体
M[:, 2]  # tensor([3, 7, 11])  - 3列目全体
```

---

## 1.4 テンソル（Tensor）

### 1.4.1 多次元配列

ベクトル（1次元）と行列（2次元）を一般化したものが**テンソル**です。

```
0次元テンソル（スカラー）:
  5

1次元テンソル（ベクトル）:
  [1, 2, 3]

2次元テンソル（行列）:
  [[1, 2, 3],
   [4, 5, 6]]

3次元テンソル:
  [[[1, 2],
    [3, 4]],
   [[5, 6],
    [7, 8]]]

4次元テンソル（nanochatで頻出）:
  バッチサイズ × シーケンス長 × ヘッド数 × ヘッド次元
  例: (32, 2048, 8, 128)
```

### 1.4.2 テンソルの形状

nanochatで頻出するテンソルの形状：

| 名前 | 形状 | 説明 |
|------|------|------|
| **入力トークンID** | `(B, T)` | B: バッチサイズ、T: シーケンス長 |
| **埋め込みベクトル** | `(B, T, D)` | D: モデル次元（n_embd） |
| **Attention重み** | `(B, H, T, T)` | H: ヘッド数 |
| **Key/Value** | `(B, H, T, d)` | d: ヘッド次元（D/H） |
| **重み行列** | `(D_in, D_out)` | 線形層の重み |

**例**：
```python
import torch

# バッチサイズ2、シーケンス長3、モデル次元4
embeddings = torch.randn(2, 3, 4)

print(embeddings.shape)  # torch.Size([2, 3, 4])
print(embeddings.ndim)   # 3  - 3次元テンソル

# 形状の各次元にアクセス
B, T, D = embeddings.shape
print(f"Batch size: {B}, Sequence length: {T}, Model dim: {D}")
# Batch size: 2, Sequence length: 3, Model dim: 4
```

---

## 1.5 基本的な演算

### 1.5.1 要素ごとの演算

**要素ごとの演算**（Element-wise operations）は、同じ位置の要素同士を演算します。

```python
import torch

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# 要素ごとの加算
print(a + b)  # tensor([5, 7, 9])
# 計算: [1+4, 2+5, 3+6] = [5, 7, 9]

# 要素ごとの乗算
print(a * b)  # tensor([4, 10, 18])
# 計算: [1*4, 2*5, 3*6] = [4, 10, 18]

# 行列でも同様
M1 = torch.tensor([[1, 2], [3, 4]])
M2 = torch.tensor([[5, 6], [7, 8]])

print(M1 + M2)
# tensor([[ 6,  8],
#         [10, 12]])
```

### 1.5.2 ベクトルの内積

**内積**（Dot product）は、2つのベクトルから1つのスカラー値を計算します。

```
数学的定義:
a = [a1, a2, a3]
b = [b1, b2, b3]

a · b = a1*b1 + a2*b2 + a3*b3
```

**例**：
```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# 内積
dot_product = torch.dot(a, b)
print(dot_product)  # tensor(32)
# 計算: 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32

# 手動計算（同じ結果）
manual = (a * b).sum()
print(manual)  # tensor(32)
```

**内積の意味**：
- ベクトルの類似度を測る（コサイン類似度の基礎）
- 2つのベクトルが似ていれば大きな値
- 直交していれば0

### 1.5.3 行列とベクトルの積

**行列とベクトルの積**は、行列の各行とベクトルの内積を計算します。

```
行列 M (2, 3) とベクトル v (3,) の積:

    ┌          ┐   ┌   ┐
    │ 1  2  3  │   │ 4 │
M = │ 5  6  7  │ × │ 5 │ = ?
    └          ┘   │ 6 │
                   └   ┘

結果（2次元ベクトル）:
  ┌                          ┐
  │ 1*4 + 2*5 + 3*6 = 32     │
  │ 5*4 + 6*5 + 7*6 = 92     │
  └                          ┘
```

```python
M = torch.tensor([[1, 2, 3],
                  [5, 6, 7]])
v = torch.tensor([4, 5, 6])

result = M @ v  # または torch.matmul(M, v)
print(result)  # tensor([32, 92])

print(result.shape)  # torch.Size([2])
```

**形状の規則**：
```
(m, n) × (n,) → (m,)
```

### 1.5.4 行列と行列の積

**行列積**（Matrix multiplication）は、最も重要な演算の1つです。

```
行列 A (2, 3) と行列 B (3, 4) の積:

    ┌          ┐   ┌              ┐
A = │ 1  2  3  │   │ 1  2  3  4   │
    │ 4  5  6  │ × │ 5  6  7  8   │ = C (2, 4)
    └          ┘   │ 9 10 11 12   │
                   └              ┘

結果 C の (i, j) 要素 = A の i 行目と B の j 列目の内積

C[0, 0] = 1*1 + 2*5 + 3*9 = 38
C[0, 1] = 1*2 + 2*6 + 3*10 = 44
...
```

```python
A = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])  # (2, 3)

B = torch.tensor([[1, 2, 3, 4],
                  [5, 6, 7, 8],
                  [9, 10, 11, 12]])  # (3, 4)

C = A @ B  # または torch.matmul(A, B)
print(C)
# tensor([[38, 44, 50, 56],
#         [83, 98, 113, 128]])

print(C.shape)  # torch.Size([2, 4])
```

**形状の規則**：
```
(m, n) × (n, p) → (m, p)
      ↑   ↑
   一致する必要がある
```

**重要な注意**：
- 行列積は**順序が重要**（非可換）: `A @ B ≠ B @ A`
- 内側の次元が一致する必要がある

---

## 1.6 転置（Transpose）

**転置**は、行列の行と列を入れ替える操作です。

```
元の行列 M (3, 4):
    ┌              ┐
    │ 1  2  3  4   │
M = │ 5  6  7  8   │
    │ 9 10 11 12   │
    └              ┘

転置 M^T (4, 3):
      ┌          ┐
      │ 1  5  9  │
M^T = │ 2  6 10  │
      │ 3  7 11  │
      │ 4  8 12  │
      └          ┘
```

```python
M = torch.tensor([[1, 2, 3, 4],
                  [5, 6, 7, 8],
                  [9, 10, 11, 12]])

# 転置
M_T = M.T  # または M.transpose(0, 1)
print(M_T)
# tensor([[ 1,  5,  9],
#         [ 2,  6, 10],
#         [ 3,  7, 11],
#         [ 4,  8, 12]])

print(M.shape)    # torch.Size([3, 4])
print(M_T.shape)  # torch.Size([4, 3])
```

**多次元テンソルの転置**：

```python
# 3次元テンソル (2, 3, 4)
tensor = torch.randn(2, 3, 4)

# 最後の2次元を転置
transposed = tensor.transpose(-1, -2)  # または tensor.transpose(1, 2)
print(transposed.shape)  # torch.Size([2, 4, 3])

# すべての次元を逆順
permuted = tensor.permute(2, 1, 0)
print(permuted.shape)  # torch.Size([4, 3, 2])
```

---

## 1.7 ブロードキャスティング

**ブロードキャスティング**は、形状の異なるテンソル同士の演算を可能にする仕組みです。

```python
# スカラーとベクトル
v = torch.tensor([1, 2, 3])
result = v + 10
print(result)  # tensor([11, 12, 13])
# 10が自動的に [10, 10, 10] に拡張される

# ベクトルと行列
M = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])  # (2, 3)
v = torch.tensor([10, 20, 30])  # (3,)

result = M + v
print(result)
# tensor([[11, 22, 33],
#         [14, 25, 36]])
# v が各行に加算される
```

**ブロードキャスティングの規則**：

1. 末尾から次元を比較
2. 次元が1または一致している場合、ブロードキャスト可能
3. 次元が存在しない場合、1として扱う

```
例1: (2, 3) + (3,)
     (2, 3) + (1, 3)  ← (3,)を(1, 3)に拡張
     (2, 3) + (2, 3)  ← (1, 3)を(2, 3)にブロードキャスト
     OK!

例2: (4, 1, 3) + (3,)
     (4, 1, 3) + (1, 1, 3)
     (4, 1, 3) + (4, 1, 3)
     OK!

例3: (2, 3) + (2,)
     末尾が3と2で一致しない
     NG!
```

**nanochatでの使用例**：

```python
# バイアスの加算 (gpt.py:89)
# x: (B, T, D)
# bias: (D,)
x = x + bias  # バイアスが各トークン、各バッチに加算される
```

---

## 1.8 nanochatでの使用例

### 埋め込み層

```python
# gpt.py:271
# idx: (B, T) - トークンID
# wte.weight: (vocab_size, n_embd) - 埋め込み重み行列

x = self.transformer.wte(idx)  # (B, T, n_embd)

# 各トークンIDが n_embd 次元のベクトルに変換される
```

**仕組み**：
```
例: vocab_size=5, n_embd=3, idx = [[1, 2], [0, 4]]

埋め込み行列:
    ┌                ┐
    │ 0.1  0.2  0.3  │  ← トークン0の埋め込み
    │ 0.4  0.5  0.6  │  ← トークン1
    │ 0.7  0.8  0.9  │  ← トークン2
    │ 1.0  1.1  1.2  │  ← トークン3
    │ 1.3  1.4  1.5  │  ← トークン4
    └                ┘

結果: (2, 2, 3)
    [[[0.4, 0.5, 0.6],   # トークン1の埋め込み
      [0.7, 0.8, 0.9]],  # トークン2の埋め込み
     [[0.1, 0.2, 0.3],   # トークン0の埋め込み
      [1.3, 1.4, 1.5]]]  # トークン4の埋め込み
```

### 線形変換（Linear層）

```python
# gpt.py:148
# x: (B, T, n_embd)
# c_fc1.weight: (4*n_embd, n_embd)

y = self.c_fc1(x)  # (B, T, 4*n_embd)

# 内部では行列積: x @ c_fc1.weight.T + bias
```

**仕組み**：
```
x: (B, T, D)
W: (D, 4D)  ← 実際は (4D, D) の転置

各トークンのベクトル (D,) が重み行列 (D, 4D) と積をとり
(4D,) のベクトルに変換される

バッチ全体では:
(B, T, D) @ (D, 4D) → (B, T, 4D)
```

### Attention計算

```python
# gpt.py:115-123
# Q, K: (B, H, T, d)

# Q @ K^T: (B, H, T, d) @ (B, H, d, T) → (B, H, T, T)
attn_weights = Q @ K.transpose(-2, -1)  # (B, H, T, T)

# V: (B, H, T, d)
# attn_weights @ V: (B, H, T, T) @ (B, H, T, d) → (B, H, T, d)
output = attn_weights @ V  # (B, H, T, d)
```

**意味**：
- `Q @ K.T`: 各トークンペア間の類似度（Attention重み）
- `attn_weights @ V`: 類似度で重み付けされたValueの加重和

### RMSNorm

```python
# gpt.py:26-27
def norm(x):
    return F.rms_norm(x, (x.size(-1),))

# x: (B, T, D)
# 各トークンのベクトル (D,) を正規化
```

---

## 1.9 まとめ

この章では、ベクトルと行列の基礎について学びました。

### 重要な概念

1. **ベクトル**
   - 1次元の数値配列
   - 形状: `(n,)`
   - 用途: 埋め込みベクトル、特徴量

2. **行列**
   - 2次元の数値配列
   - 形状: `(m, n)`
   - 用途: 重み行列、データバッチ

3. **テンソル**
   - 多次元配列の一般化
   - 形状: `(d1, d2, ..., dn)`
   - 用途: すべてのニューラルネットワークのデータ

4. **基本演算**
   - 要素ごとの演算: `+`, `*`, `-`, `/`
   - 内積: `torch.dot(a, b)`
   - 行列積: `A @ B`
   - 転置: `M.T`

5. **形状の規則**
   ```
   行列とベクトル: (m, n) × (n,) → (m,)
   行列と行列:     (m, n) × (n, p) → (m, p)
   ```

6. **ブロードキャスティング**
   - 形状の異なるテンソル同士の演算
   - 末尾から次元を比較
   - 1または一致している場合に拡張

### nanochatでの主な使用箇所

| 操作 | コード位置 | 説明 |
|------|----------|------|
| 埋め込み | `gpt.py:271` | トークンID → ベクトル |
| 線形変換 | `gpt.py:148` | `x @ W.T + b` |
| Attention | `gpt.py:115-123` | `Q @ K.T`, `attn @ V` |
| 正規化 | `gpt.py:26-27` | ベクトルごとの正規化 |

### 次のステップ

次の数学ドキュメントでは、以下を学びます：
- **行列演算の詳細**: アダマール積、クロネッカー積、行列の分解
- **Softmax関数**: 確率分布への変換
- **Attention機構の数式**: Query, Key, Valueの詳細な計算

ベクトルと行列の基礎を理解したことで、これらのより高度な概念を学ぶ準備ができました。

---

**関連ドキュメント**:
- [数学02: 行列演算](02-matrix-operations.md)
- [数学07: Attention機構の数式](07-attention-math.md)
- [第4章: モデルの詳細実装](../04-model-implementation.md)
