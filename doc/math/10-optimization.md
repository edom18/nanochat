# 第10章: 最適化アルゴリズム

## 目次
- [最適化の基礎](#最適化の基礎)
- [勾配降下法の変種](#勾配降下法の変種)
  - [Batch Gradient Descent](#batch-gradient-descent)
  - [Stochastic Gradient Descent (SGD)](#stochastic-gradient-descent-sgd)
  - [Mini-Batch Gradient Descent](#mini-batch-gradient-descent)
- [モーメンタムベースの最適化](#モーメンタムベースの最適化)
  - [Momentum](#momentum)
  - [Nesterov Accelerated Gradient](#nesterov-accelerated-gradient)
- [適応的学習率](#適応的学習率)
  - [AdaGrad](#adagrad)
  - [RMSProp](#rmsprop)
  - [Adam](#adam)
  - [AdamW](#adamw)
- [二次最適化手法](#二次最適化手法)
  - [Muon](#muon)
- [比較とまとめ](#比較とまとめ)
- [nanochatでの実装](#nanochatでの実装)
- [練習問題](#練習問題)

---

## 最適化の基礎

**最適化**の目標は、損失関数 `L(θ)` を最小化するパラメータ `θ` を見つけることです。

### 数式の再確認

```
目的: min L(θ)
      θ

更新則（勾配降下法）:
  θ_{t+1} = θ_t - η ∇L(θ_t)

ここで:
  θ_t: 時刻tのパラメータ
  η: 学習率
  ∇L(θ_t): 損失の勾配
```

詳細は [数学05: 勾配降下法](05-gradient-descent.md) を参照。

### 最適化の課題

1. **収束速度**: 早く最適解に到達したい
2. **勾配消失/爆発**: 深いネットワークで勾配が不安定
3. **鞍点問題**: 平らな領域で更新が停滞
4. **学習率の選択**: 大きすぎると発散、小さすぎると遅い
5. **パラメータごとの最適学習率**: 層やパラメータごとに異なる

---

## 勾配降下法の変種

### Batch Gradient Descent

**全データセット**を使って勾配を計算します。

#### 数式

```
θ_{t+1} = θ_t - η (1/N) Σ_{i=1}^N ∇L_i(θ_t)

ここで:
  N: データセットのサイズ
  L_i: i番目のサンプルの損失
```

#### 利点と欠点

**利点**:
- 安定した収束（勾配の分散が小さい）
- 理論的に保証された収束

**欠点**:
- 計算コストが高い（全データを毎回処理）
- メモリ使用量が大きい
- 更新頻度が低い（エポックごと）

### Stochastic Gradient Descent (SGD)

**1サンプルずつ**使って勾配を計算します。

#### 数式

```
各ステップで1サンプル i をランダム選択:
  θ_{t+1} = θ_t - η ∇L_i(θ_t)
```

#### 利点と欠点

**利点**:
- 高速（1サンプルのみ処理）
- 更新頻度が高い
- ノイズが局所最適解から脱出を助ける

**欠点**:
- 勾配の分散が大きい（ノイジー）
- 収束が不安定

### Mini-Batch Gradient Descent

**小さなバッチ**を使って勾配を計算します（**最も一般的**）。

#### 数式

```
バッチ B_t（サイズ m）をランダム選択:
  θ_{t+1} = θ_t - η (1/m) Σ_{i∈B_t} ∇L_i(θ_t)

通常 m = 32, 64, 128, 256 など
```

#### 利点

- **バランス**: 安定性と効率のバランス
- **並列化**: GPUで効率的に計算可能
- **汎化**: 適度なノイズが汎化性能を向上

**nanochatはMini-Batch GDを使用**しています。

---

## モーメンタムベースの最適化

### Momentum

**過去の勾配を蓄積**して、更新に慣性を持たせます。

#### 動機

```
問題: 勾配降下法は谷間でジグザグに進む

  ↓ 勾配方向
  |\ /\ /\ /\
  | X  X  X  X  ← ジグザグの軌跡
  |/\ /\ /\ /\
  v 最適解
```

モーメンタムにより、谷の方向に加速し、横方向の振動を抑制します。

#### 数式

```
v_t = β v_{t-1} + ∇L(θ_t)
θ_{t+1} = θ_t - η v_t

ここで:
  v_t: 速度（モーメンタム）
  β: 減衰係数（通常 0.9）
```

展開すると：
```
v_t = β v_{t-1} + ∇L(θ_t)
    = β (β v_{t-2} + ∇L(θ_{t-1})) + ∇L(θ_t)
    = β² v_{t-2} + β ∇L(θ_{t-1}) + ∇L(θ_t)
    = ...
    = Σ_{i=0}^t β^i ∇L(θ_{t-i})

過去の勾配の指数移動平均
```

#### 効果

```
β = 0.9 の場合:
  現在の勾配: 重み 1.0
  1ステップ前: 重み 0.9
  2ステップ前: 重み 0.81
  3ステップ前: 重み 0.729
  ...

過去の方向を記憶しつつ、新しい情報を取り入れる
```

### Nesterov Accelerated Gradient

**先読み勾配**を使ったモーメンタムの改良版です。

#### アイデア

```
通常のモーメンタム:
  1. 現在位置で勾配計算
  2. モーメンタムと勾配を合わせて移動

Nesterov:
  1. モーメンタム方向に先に移動
  2. その先の位置で勾配計算
  3. 合わせて最終位置を決定

「未来を見てから方向を調整」
```

#### 数式

```
v_t = β v_{t-1} + ∇L(θ_t - η β v_{t-1})
                           ↑
                      先読み位置で勾配計算
θ_{t+1} = θ_t - η v_t
```

#### 効果

谷底に近づくと、先読み勾配が警告を発し、オーバーシュートを防ぎます。

---

## 適応的学習率

### AdaGrad

**パラメータごとに学習率を調整**します。

#### 動機

```
問題: 全パラメータに同じ学習率を使うのは非効率

例:
  頻繁に更新されるパラメータ → 小さい学習率が望ましい
  まれにしか更新されないパラメータ → 大きい学習率が望ましい
```

#### 数式

```
G_t = G_{t-1} + (∇L(θ_t))²
θ_{t+1} = θ_t - (η / √(G_t + ε)) ∇L(θ_t)

ここで:
  G_t: 勾配の二乗和（累積）
  ε: ゼロ除算防止（例: 1e-8）
  要素ごとの演算
```

#### 効果

```
大きな勾配を多く受けたパラメータ:
  G_t が大きい → 学習率が小さくなる

小さな勾配しか受けていないパラメータ:
  G_t が小さい → 学習率が大きいまま
```

#### 問題点

**学習率が単調減少**するため、長時間訓練すると更新が停止します。

```
G_t は常に増加 → √G_t も増加 → η/√G_t は減少し続ける
```

### RMSProp

AdaGradの問題を**指数移動平均**で解決します。

#### 数式

```
E[g²]_t = β E[g²]_{t-1} + (1-β) (∇L(θ_t))²
θ_{t+1} = θ_t - (η / √(E[g²]_t + ε)) ∇L(θ_t)

ここで:
  E[g²]_t: 勾配の二乗の指数移動平均
  β: 減衰係数（通常 0.9 or 0.99）
```

#### AdaGradとの違い

```
AdaGrad: G_t = Σ_{i=1}^t g_i²  (累積和、単調増加)
RMSProp: E[g²]_t = β E[g²]_{t-1} + (1-β) g_t²  (移動平均、安定)
```

古い勾配の影響が減衰するため、学習率が回復可能です。

### Adam

**モーメンタム** と **適応的学習率** を組み合わせた手法です（最も人気）。

#### 数式

```
1. モーメンタム（1次モーメント）:
   m_t = β_1 m_{t-1} + (1-β_1) ∇L(θ_t)

2. 勾配の二乗の移動平均（2次モーメント）:
   v_t = β_2 v_{t-1} + (1-β_2) (∇L(θ_t))²

3. バイアス補正:
   m̂_t = m_t / (1 - β_1^t)
   v̂_t = v_t / (1 - β_2^t)

4. パラメータ更新:
   θ_{t+1} = θ_t - η (m̂_t / (√v̂_t + ε))

デフォルト: β_1=0.9, β_2=0.999, ε=1e-8
```

#### バイアス補正の必要性

```
初期化: m_0 = 0, v_0 = 0

ステップ1:
  m_1 = β_1 * 0 + (1-β_1) g_1 = 0.1 g_1  （β_1=0.9の場合）

実際の期待値 E[g] は g_1 のはずだが、m_1 は 0.1 g_1 と過小評価

バイアス補正:
  m̂_1 = m_1 / (1 - β_1^1) = 0.1 g_1 / 0.1 = g_1  ✓
```

時刻tが大きくなると、`β_1^t → 0` となり、補正の効果が消えます。

#### 直感的理解

```
Adamは2つの情報を使う:

1. m_t（モーメンタム）: 「どの方向に進むべきか」
   → 勾配の平均方向

2. v_t（適応的学習率）: 「どのくらい確信があるか」
   → 勾配の分散（不確実性）

更新:
  確信が高い（v小）→ 大きく進む
  確信が低い（v大）→ 慎重に進む
```

### AdamW

Adamの**Weight Decay（重み減衰）** を修正した版です。

#### 問題: AdamでのWeight Decay

通常のWeight Decayは損失関数に正則化項を追加します：

```
L_total(θ) = L(θ) + (λ/2) ||θ||²

勾配:
  ∇L_total = ∇L + λθ
```

Adamに直接適用すると：
```
m_t = β_1 m_{t-1} + (1-β_1) (∇L + λθ)
```

問題: **Weight Decayが適応的学習率の影響を受ける**
→ パラメータごとに異なる減衰率になる（意図しない）

#### AdamWの解決策

Weight Decayを**適応的学習率の外で適用**します。

```
1. モーメンタム・適応的学習率はAdamと同じ:
   m_t = β_1 m_{t-1} + (1-β_1) ∇L(θ_t)
   v_t = β_2 v_{t-1} + (1-β_2) (∇L(θ_t))²
   m̂_t = m_t / (1 - β_1^t)
   v̂_t = v_t / (1 - β_2^t)

2. パラメータ更新（Weight Decayを分離）:
   θ_{t+1} = θ_t - η (m̂_t / (√v̂_t + ε)) - η λ θ_t
                                              ↑
                                        直接的なWeight Decay
```

展開すると：
```
θ_{t+1} = θ_t - η (m̂_t / √v̂_t) - η λ θ_t
        = (1 - η λ) θ_t - η (m̂_t / √v̂_t)
          ↑
        Weight Decayによるスケーリング
```

#### 効果

```
Adam: Weight Decayが適応的学習率で調整される
  → パラメータごとに異なる減衰率

AdamW: Weight Decayが一律に適用される
  → 意図通りの正則化
```

実験的に、AdamWはAdamより汎化性能が向上することが多いです。

---

## 二次最適化手法

### Muon

**Muon**（Momentum Orthogonalized by Newton-schulz）は、**行列の直交化**を利用した最適化手法です。

詳細は [第6章: 最適化手法](../06-optimization.md) を参照してください。ここでは数学的な核心のみ説明します。

#### 動機

勾配は「最も急な下り坂」を指しますが、パラメータ空間の**曲率**を考慮していません。

```
1次最適化（SGD, Adam）:
  勾配のみを使用 → 局所的な線形近似

2次最適化:
  勾配 + 曲率（ヘッセ行列）→ より正確な方向

問題: ヘッセ行列の計算とその逆行列は非常にコストが高い
  ヘッセ行列のサイズ: (パラメータ数)²
```

#### Muonのアイデア

完全な2次情報の代わりに、**直交化**を使って勾配の方向を改善します。

```
通常の勾配更新:
  θ_{t+1} = θ_t - η g_t

Muon:
  1. モーメンタムを計算: m_t = β m_{t-1} + g_t
  2. 直交化（Newton-Schulz iteration）: m̃_t = Orthogonalize(m_t)
  3. 更新: θ_{t+1} = θ_t - η m̃_t
```

#### Newton-Schulz Iteration

行列 `A` を直交化する反復法：

```
初期化: Z_0 = A / ||A||

反復:
  Z_{k+1} = 0.5 * Z_k * (3I - Z_k^T Z_k)

収束: Z_k → 直交行列
```

**性質**:
- `Z_k^T Z_k → I`（単位行列に収束）
- 行列の「方向」を保ちつつ、直交性を強制

#### 効果

直交化により：
1. **スケール不変性**: パラメータのスケールに頑健
2. **条件数の改善**: 最適化の地形を滑らかに
3. **収束の高速化**: 特に深いネットワークで効果的

#### nanochatでの使用

```python
# Muonは内部パラメータ（Attention, MLP）に使用
# AdamWは外部パラメータ（Embedding）に使用

# 詳細は第6章を参照
```

---

## 比較とまとめ

### 最適化手法の系譜

```
基本的な勾配降下法:
  ├─ Batch GD
  ├─ SGD
  └─ Mini-Batch GD ← 実用的

モーメンタム系:
  ├─ Momentum
  └─ Nesterov

適応的学習率系:
  ├─ AdaGrad
  ├─ RMSProp
  ├─ Adam ← 最も人気
  └─ AdamW ← Adamの改良

2次最適化系:
  ├─ Newton法（完全2次）
  ├─ L-BFGS（準ニュートン法）
  └─ Muon（直交化ベース）
```

### 特性比較

| 手法 | モーメンタム | 適応的LR | 計算量 | メモリ | 汎化性能 |
|------|------------|---------|--------|--------|---------|
| SGD | ✗ | ✗ | 低 | 低 | 高（調整が必要） |
| Momentum | ✓ | ✗ | 低 | 低 | 高 |
| Adam | ✓ | ✓ | 中 | 中 | 中 |
| AdamW | ✓ | ✓ | 中 | 中 | 高 |
| Muon | ✓ | ✗ | 高 | 高 | 非常に高 |

### 選択のガイドライン

```
デフォルトの選択:
  AdamW (β_1=0.9, β_2=0.999, weight_decay=0.01)
  理由: 安定、調整が容易、広く使われている

高速収束が必要:
  Muon (大規模モデルで特に効果的)
  理由: 2次情報の近似により収束が早い

最高の汎化性能:
  SGD + Momentum (学習率スケジューリング必須)
  理由: 適切に調整すればAdamより汎化することが多い
  欠点: ハイパーパラメータ調整が難しい
```

### ハイパーパラメータの典型値

```
Adam/AdamW:
  学習率: 1e-4 〜 5e-4（Transformerの場合）
  β_1: 0.9
  β_2: 0.999
  weight_decay: 0.01 〜 0.1
  ε: 1e-8

SGD + Momentum:
  学習率: 0.01 〜 0.1（warmupとdecayが必須）
  momentum: 0.9 〜 0.99

Muon:
  学習率: 0.01 〜 0.1
  momentum: 0.95
  Newton-Schulz反復: 5回
```

---

## nanochatでの実装

nanochatは**Muon + AdamW のハイブリッド**を使用します。

### パラメータの分割

```python
# 内部パラメータ（Attention, MLP）: Muon
muon_params = []

# 外部パラメータ（Embedding, LM Head）: AdamW
adamw_params = []

# 詳細は第6章を参照
```

### 理由

```
Muon:
  内部パラメータは2次最適化の恩恵が大きい
  行列の直交化が効果的

AdamW:
  Embeddingは疎な更新（頻繁に更新されるトークンとされないトークンがある）
  適応的学習率が有効
```

### 実装の参照

完全な実装は以下を参照：
- [第6章: 最適化手法](../06-optimization.md)
- `nanochat/muon.py`
- `scripts/base_train.py`

---

## 練習問題

### 問題1: モーメンタムの計算

`β=0.9` のMomentumで、以下の勾配列を処理してください（初期 `v_0=0`）。

```
ステップ1: g_1 = 2.0
ステップ2: g_2 = 1.0
ステップ3: g_3 = 3.0
```

<details>
<summary>解答</summary>

```
v_t = β v_{t-1} + g_t

ステップ1:
  v_1 = 0.9 * 0 + 2.0 = 2.0

ステップ2:
  v_2 = 0.9 * 2.0 + 1.0 = 1.8 + 1.0 = 2.8

ステップ3:
  v_3 = 0.9 * 2.8 + 3.0 = 2.52 + 3.0 = 5.52

過去の勾配が蓄積され、加速している
```
</details>

### 問題2: Adamのバイアス補正

`β_1=0.9`, `m_0=0`, `g_1=1.0` のとき、補正前の `m_1` と補正後の `m̂_1` を計算してください。

<details>
<summary>解答</summary>

```
1. モーメンタム:
   m_1 = β_1 m_0 + (1-β_1) g_1
       = 0.9 * 0 + 0.1 * 1.0
       = 0.1

2. バイアス補正:
   m̂_1 = m_1 / (1 - β_1^1)
        = 0.1 / (1 - 0.9)
        = 0.1 / 0.1
        = 1.0

補正により、真の期待値 g_1 に一致！
```
</details>

### 問題3: AdamとAdamWのWeight Decay

パラメータ `θ=1.0`, 学習率 `η=0.1`, weight decay `λ=0.01` の場合、Adam とAdamWでのWeight Decay適用後の違いを比較してください（簡略化のため、`m̂/√v̂ = 0.5` と仮定）。

<details>
<summary>解答</summary>

```
Adam（Weight DecayをL2正則化として損失に追加）:
  ∇L_total = ∇L + λθ = (m̂/√v̂) + 0.01 * 1.0 = 0.5 + 0.01 = 0.51
  θ_new = θ - η ∇L_total
        = 1.0 - 0.1 * 0.51
        = 1.0 - 0.051
        = 0.949

AdamW（Weight Decayを直接適用）:
  θ_new = θ - η (m̂/√v̂) - η λ θ
        = 1.0 - 0.1 * 0.5 - 0.1 * 0.01 * 1.0
        = 1.0 - 0.05 - 0.001
        = 0.949

この例では同じだが、適応的学習率の影響がパラメータごとに異なる場合、
AdamWの方が一律に減衰して意図通りの正則化になる
```
</details>

### 問題4: 最適化手法の選択

以下の状況で、どの最適化手法を選ぶべきか理由とともに答えてください。

1. 小規模モデル、素早くプロトタイプを作りたい
2. 大規模Transformer、最高の性能が必要
3. 予算が限られており、計算資源が少ない

<details>
<summary>解答</summary>

```
1. 小規模モデル、プロトタイプ:
   選択: Adam または AdamW
   理由: ハイパーパラメータ調整が容易、デフォルト値で動作、実装が簡単

2. 大規模Transformer、最高性能:
   選択: Muon + AdamW（nanochatのアプローチ）
   理由: Muonの2次最適化が収束を加速、AdamWで外部パラメータを安定化

3. 計算資源が限られている:
   選択: AdamW（または SGD + Momentum）
   理由: Muonは追加の計算とメモリが必要、AdamWは標準的で効率的
```
</details>

---

## まとめ

### 最適化手法の進化

```
1次最適化（勾配のみ）:
  SGD → Momentum → Adam → AdamW
  特徴: 計算効率的、広く使われる

2次最適化（曲率も考慮）:
  Newton法 → L-BFGS → Muon
  特徴: 収束が早い、計算コストが高い
```

### 核心となる考え方

```
1. モーメンタム: 過去の勾配を蓄積して加速
   v_t = β v_{t-1} + g_t

2. 適応的学習率: パラメータごとに学習率を調整
   η_i = η / √(v_i + ε)

3. Weight Decay: 正則化を適切に適用
   θ ← (1 - ηλ) θ - η g

4. 直交化（Muon）: 勾配の方向を改善
   g̃ = Orthogonalize(g)
```

### nanochatの戦略

```
ハイブリッド最適化:
  内部パラメータ（Attention, MLP）→ Muon
  外部パラメータ（Embedding）→ AdamW

理由:
  収束速度と安定性のバランス
```

### 実用的なアドバイス

```
初心者:
  AdamWを使う（デフォルト設定で開始）

経験者:
  タスクに応じて選択:
    - 速度重視 → Muon
    - 汎化重視 → SGD + Momentum（要調整）
    - バランス → AdamW
```

### 次のステップ

- [数学05: 勾配降下法](05-gradient-descent.md) - 基礎的な勾配降下法
- [数学06: 誤差逆伝播法](06-backpropagation.md) - 勾配の計算方法
- [第6章: 最適化手法](../06-optimization.md) - nanochatでの実装詳細
